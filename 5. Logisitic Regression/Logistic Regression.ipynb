{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Basics :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a machine learning algorithm used for classification problem. It is based on concept of probability and one of the best and simple method for predictive analysis.\n",
    "Logistic regression is modified version of simple linear regression. Logistic regression uses much complex cost function (called sigmoid function) than linear regression as it produce values between 0 and 1 while linear regression can produce anything between negative infinity to positive infinity.As through logistic regression we calculate the probabilities and the probabilities are bounded in nature between 0 and 1 , we calculate odds or odds ratioof an event which is defined as \n",
    "\n",
    "\\begin{align}\n",
    "odds= \\frac{p}{1-p}= \\frac{probability of event}{1- probability of event}\n",
    "\\end{align}\n",
    "\n",
    "odds has a range from 0 to positive infinity\n",
    "\n",
    "where p= P(Y=1)\n",
    "In logistic regression we take the logit or log odds of events being linear to the predictor variables. We calculate the the logs to scale down the odds value\n",
    "\n",
    "\\begin{align}\n",
    "z =ln(\\frac{p}{1-p})=\\beta_{0}x_{0}+\\beta_{1}x_{1}+\\beta_{2}x_{2}+\\beta_{3}x_{3}+------+\\beta_{n}x_{n}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Dependent variable has to be categorical in nature\n",
    "\n",
    "b. P(Y=1)is the probability of occurrence of event where Y is the target variable\n",
    "\n",
    "c. Error terms need to be independent.Logistic regression requires each observation to be independent.\n",
    "\n",
    "d. Model should have little or nomulticollinearity\n",
    "\n",
    "e. Logistic regression assumes linearity of independent variables and logodds\n",
    "\n",
    "f. Sample size should be large enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification can be of text data, tabular (structured) data, image classification and some more. Classification is majorly of two broader types\n",
    "\n",
    "1. Binary:only 2 classes- (pass , fail) which can be labled as 0 and 1\n",
    "\n",
    "2. Multi class: More than 2 Classes- (Low, Medium , High) Can be represented as 0, 1 and 2\n",
    "\n",
    "Some of the examples of classification problems are, Emails can be spam/non spam, online transactions can be fraud or valid, Tumor malignant or benign. Logistic regression uses labeled data for training and use a sigmoid function to return the probabilities in the end, which can be used to determine the class of a particular test input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis Representation :\n",
    "For linear regression we use following equation for hypothesis.\n",
    "\\begin{align}\n",
    "y = w^T x+b\n",
    "\\end{align}\n",
    " \n",
    "Where wT is the transpose of weight(Coefficient) matrix and b is bias, for the simplicity we can consider it as equation of line y=mx+c.\n",
    "For logistic regression we modify it in the form of a sigmoid functtion.sigmoid function returns the probability from logit function.\n",
    "\n",
    "\\begin{align}\n",
    "y ̂=σ(w^T x+b)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "σ(z)=  1/(1+\\mathrm{e}^{-z} )\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "y ̂=1/(1+e^(-(w^T x+b)))\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def sigmoid(z):\n",
    "  return 1.0 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundary :"
   ]
  },
  {
   "attachments": {
    "decision%20boundary.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our current prediction function returns a probability score between 0 and 1. In order to map this to a discrete class (true/false, cat/dog), we select a threshold value or tipping point above which we will classify values into class 1 and below which we classify values into class 2.\n",
    "\n",
    "p≥0.5,class=1\n",
    "\n",
    "p<0.5,class=0\n",
    "\n",
    "For example, if our threshold was .5 and our prediction function returned .7, we would classify this observation as positive. If our prediction was .2 we would classify the observation as negative. For logistic regression with multiple classes we could select the class with the highest predicted probability.\n",
    "\n",
    "![decision%20boundary.png](attachment:decision%20boundary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Code for Prediction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(features, weights):\n",
    "  '''\n",
    "  Returns 1D array of probabilities\n",
    "  that the class label == 1\n",
    "  '''\n",
    " \n",
    "  z = np.dot(features,weights) #Gives the logit or log of odds function mentioned in the beginning of the article\n",
    "  x=sigmoid(z)\n",
    "      return x#sigmoid function converts the logit into probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Dataset For Implementation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset:\n",
    "The dataset contains three independent variables gmat, gpa and work_experiencewhich stands for GMAT score, GPA score and workexperience of candidates. The target variable is 'admitted' - which stands for the status of candidates if they got an admit or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    gmat  gpa  work_experience  admitted\n",
      "0    780  4.0                3         1\n",
      "1    750  3.9                4         1\n",
      "2    690  3.3                3         1\n",
      "3    710  3.7                5         1\n",
      "4    680  3.9                4         1\n",
      "5    730  3.7                6         1\n",
      "6    690  2.3                1         0\n",
      "7    720  3.3                4         1\n",
      "8    740  3.3                5         1\n",
      "9    690  1.7                1         0\n",
      "10   610  2.7                3         0\n",
      "11   690  3.7                5         1\n",
      "12   710  3.7                6         1\n",
      "13   680  3.3                4         1\n",
      "14   770  3.3                3         1\n",
      "15   610  3.0                1         0\n",
      "16   580  2.7                4         0\n",
      "17   650  3.7                6         1\n",
      "18   540  2.7                2         0\n",
      "19   590  2.3                3         0\n",
      "20   620  3.3                2         0\n",
      "21   600  2.0                1         0\n",
      "22   550  2.3                4         0\n",
      "23   550  2.7                1         0\n",
      "24   570  3.0                2         0\n",
      "25   670  3.3                6         1\n",
      "26   660  3.7                4         1\n",
      "27   580  2.3                2         0\n",
      "28   650  3.7                6         1\n",
      "29   660  3.3                5         1\n",
      "30   640  3.0                1         0\n",
      "31   620  2.7                2         0\n",
      "32   660  4.0                4         1\n",
      "33   660  3.3                6         1\n",
      "34   680  3.3                5         1\n",
      "35   650  2.3                1         0\n",
      "36   670  2.7                2         0\n",
      "37   580  3.3                1         0\n",
      "38   590  1.7                4         0\n",
      "39   690  3.7                5         1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "candidates = {'gmat': [780,750,690,710,680,730,690,720,740,690,610,690,710,680,770,610,580,650,540,590,620,600,550,550,570,670,660,580,650,660,640,620,660,660,680,650,670,580,590,690],\n",
    "              'gpa': [4,3.9,3.3,3.7,3.9,3.7,2.3,3.3,3.3,1.7,2.7,3.7,3.7,3.3,3.3,3,2.7,3.7,2.7,2.3,3.3,2,2.3,2.7,3,3.3,3.7,2.3,3.7,3.3,3,2.7,4,3.3,3.3,2.3,2.7,3.3,1.7,3.7],\n",
    "              'work_experience': [3,4,3,5,4,6,1,4,5,1,3,5,6,4,3,1,4,6,2,3,2,1,4,1,2,6,4,2,6,5,1,2,4,6,5,1,2,1,4,5],\n",
    "              'admitted': [1,1,1,1,1,1,0,1,1,0,0,1,1,1,1,0,0,1,0,0,0,0,0,0,0,1,1,0,1,1,0,0,1,1,1,0,0,0,0,1]\n",
    "              }\n",
    "\n",
    "df = pd.DataFrame(candidates,columns= ['gmat', 'gpa','work_experience','admitted'])\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of cost function is to be minimized, so that model we prepared will have better accuracy and minimum total error.\n",
    "In linear regression we use sum of squared error as our cost function but that cannot be used for logistic regression. If we try to use linear regression cost function for logistic regression, it will end up creating a non-convex function with many local optima(minima and maxima) and it will be very difficult to minimize cost function to get global minima.\n",
    "\n",
    "So, the loss function for logistic regression can be defined as\n",
    "\n",
    "\\begin{align}\n",
    "L(y ,\\hat{y})= -ylog(\\hat{y}) -(1-y)log⁡(1-\\hat{y})\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "If y=1; \n",
    "\n",
    "\\begin{align}\n",
    "L(y,\\hat{y}) = -log(\\hat{y})\n",
    "\\end{align}\n",
    "Intuition is, \n",
    "We want log(ŷ) large, so we want ŷ to be large, which can’t be greater than 1, because of sigmoid.\n",
    " \n",
    "If y=0;  \n",
    "\\begin{align}\n",
    "L(y,\\hat{y}) = -log(1-\\hat{y})\n",
    "\\end{align}\n",
    "We want log(1-ŷ) large, so we want ŷ to be small, which can’t be smaller than 0, because of sigmoid.\n",
    "\n",
    "It can be written as,\n",
    "\n",
    "\\begin{align}\n",
    "J(w,b)= \\frac{1}{m}\\sum_{i=1}^{m} L(\\hat{y}_i,y_i)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "J(w,b)= -\\frac{1}{m}\\sum_{i=1}^{m} (y_i log(\\hat{y}_i)+(1-y_i)log(1-\\hat{y}_i)\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code For Cost Function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_function(features, target, weights):\n",
    "    '''\n",
    "    Using Mean Absolute Error\n",
    "\n",
    "    Features:(100,3)\n",
    "    Labels: (100,1)\n",
    "    Weights:(3,1)\n",
    "    Returns 1D matrix of predictions\n",
    "    Cost = (labels*log(predictions) + (1-labels)*log(1-predictions) ) / len(labels)\n",
    "    '''\n",
    "    observations = len(target)\n",
    "\n",
    "    predictions = predict(features, weights)\n",
    "\n",
    "    #Take the error when label=1\n",
    "    class1_cost = -np.dot(target.T,np.log(predictions))\n",
    "\n",
    "    #Take the error when label=0\n",
    "    class2_cost = np.dot((1-target).T,np.log(1-predictions))\n",
    "\n",
    "    #Take the sum of both costs\n",
    "    cost = class1_cost - class2_cost\n",
    "\n",
    "    #Take the average cost\n",
    "    cost = cost.sum() / observations\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize our cost, we use Gradient Descent just like before in Linear Regression. There are other more sophisticated optimization algorithms out there such as conjugate gradient like BFGS, but you don’t have to worry about these. Machine learning libraries like Scikit-learn hide their implementations so you can focus on more interesting things!\n",
    "Math\n",
    "One of the neat properties of the sigmoid function is its derivative is easy to calculate. So as before we can calculate the partial derivatives of the \n",
    "\\begin{align}\n",
    "s^{\\prime}(z)=s(z)(1−s(z))\n",
    "\\end{align}\n",
    "\n",
    "Which leads to an equally beautiful and convenient cost function derivative:\n",
    "\n",
    "\\begin{align}\n",
    " C^{\\prime} =x(s(z)−y)\n",
    "\\end{align}\n",
    "\n",
    "Here s(z) is basically the prediction, y is the target and x is the feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Gradient and coefficient estimation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in log\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 nan [[-0.19375  ]\n",
      " [-0.00165  ]\n",
      " [-0.0038125]]\n",
      "100 16.9812683031 [[ 1.44125  ]\n",
      " [ 0.0060875]\n",
      " [ 0.00475  ]]\n",
      "200 nan [[ 1.44125  ]\n",
      " [ 0.0060875]\n",
      " [ 0.00475  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(          0\n",
       " 0 -0.832956\n",
       " 1  0.215385\n",
       " 2  0.831618,\n",
       " [nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  314.53058409374989,\n",
       "  nan,\n",
       "  172.77739146874981,\n",
       "  nan,\n",
       "  31.02419884374974,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  274.67818662499957,\n",
       "  nan,\n",
       "  132.92499399999951,\n",
       "  nan,\n",
       "  6.9065510758217759,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  234.82537503382872,\n",
       "  nan,\n",
       "  93.072182408828638,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  inf,\n",
       "  nan,\n",
       "  194.97297756507845,\n",
       "  nan,\n",
       "  53.219784940078355,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  296.87377272132818,\n",
       "  nan,\n",
       "  155.12058009632813,\n",
       "  nan,\n",
       "  13.367387471572005,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  257.02137550579249,\n",
       "  nan,\n",
       "  115.26818288079241,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  inf,\n",
       "  nan,\n",
       "  217.16897803704222,\n",
       "  nan,\n",
       "  75.41578541204214,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  319.06977319329201,\n",
       "  nan,\n",
       "  177.3165805682919,\n",
       "  nan,\n",
       "  35.563387943291843,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  279.21737572454168,\n",
       "  nan,\n",
       "  137.4641830995416,\n",
       "  nan,\n",
       "  3.1535998106335299,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  238.43925586560272,\n",
       "  nan,\n",
       "  96.686063240602664,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  inf,\n",
       "  nan,\n",
       "  198.58685839685245,\n",
       "  nan,\n",
       "  56.83366577185236,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  300.48765355310218,\n",
       "  nan,\n",
       "  158.73446092810212,\n",
       "  nan,\n",
       "  16.981268303102841,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  260.63525608512566,\n",
       "  nan,\n",
       "  118.88206346012562,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  inf,\n",
       "  nan,\n",
       "  220.78285861637522,\n",
       "  nan,\n",
       "  79.029665991375168,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  322.68365377262501,\n",
       "  nan,\n",
       "  180.93046114762495,\n",
       "  nan,\n",
       "  39.177268522624878,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  282.83125630387474,\n",
       "  nan,\n",
       "  141.07806367887466,\n",
       "  nan,\n",
       "  0.54377167816147831,\n",
       "  142.69444756932504,\n",
       "  nan,\n",
       "  1.0768324428033296,\n",
       "  nan,\n",
       "  7.7110519653870355,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  251.36659424139552,\n",
       "  nan,\n",
       "  109.61340161639551,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  inf,\n",
       "  nan,\n",
       "  211.51419677264525,\n",
       "  nan,\n",
       "  69.76100414764521,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  313.41499192889506,\n",
       "  nan,\n",
       "  171.66179930389498,\n",
       "  nan,\n",
       "  29.90860667889492,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  273.56259446014479,\n",
       "  nan,\n",
       "  131.80940183514468,\n",
       "  nan,\n",
       "  7.3177276101150497,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  233.71003721604902,\n",
       "  nan,\n",
       "  91.956844591048963,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  inf,\n",
       "  nan,\n",
       "  193.85763974729875,\n",
       "  nan,\n",
       "  52.104447122298687,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  295.75843490354856,\n",
       "  nan,\n",
       "  154.00524227854845,\n",
       "  nan,\n",
       "  12.25204965443859,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  255.90603837168428,\n",
       "  nan,\n",
       "  114.15284574668422,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  inf,\n",
       "  nan,\n",
       "  216.05364090293401,\n",
       "  nan,\n",
       "  74.300448277933924,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  317.95443605918382,\n",
       "  nan,\n",
       "  176.20124343418371,\n",
       "  nan,\n",
       "  34.448050809183641,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  278.10203859043349,\n",
       "  nan,\n",
       "  136.34884596543344,\n",
       "  nan,\n",
       "  3.5640833400875174,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  237.87688431871933,\n",
       "  nan,\n",
       "  96.123691693719266,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  inf,\n",
       "  nan,\n",
       "  198.02448684996904,\n",
       "  nan,\n",
       "  56.271294224968983,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  299.92528200621882,\n",
       "  nan,\n",
       "  158.17208938121877,\n",
       "  nan,\n",
       "  16.418896756219915,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  260.07288453873252,\n",
       "  nan,\n",
       "  118.31969191373246,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  inf,\n",
       "  nan,\n",
       "  220.22048706998208,\n",
       "  nan,\n",
       "  78.467294444982002,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  322.12128222623181,\n",
       "  nan,\n",
       "  180.36808960123179,\n",
       "  nan,\n",
       "  38.614896976231719,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  282.26888475748154,\n",
       "  nan,\n",
       "  140.51569213248149,\n",
       "  nan,\n",
       "  0.49749267071498515,\n",
       "  200.29037815781334,\n",
       "  nan,\n",
       "  58.537185532813304,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  302.19117331406312])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_weights(features, target, weights, lr):\n",
    "    '''\n",
    "    Vectorized Gradient Descent\n",
    "\n",
    "    Features:(200, 3)\n",
    "    Labels: (200, 1)\n",
    "    Weights:(3, 1)\n",
    "    '''\n",
    "    N = len(features)\n",
    "\n",
    "    #1 - Get Predictions\n",
    "    predictions = predict(features, weights)\n",
    "    #pred=pd.DataFrame(predictions)\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "    #2 Transpose features from (200, 3) to (3, 200)\n",
    "    # So we can multiply w the (200,1)  cost matrix.\n",
    "    # Returns a (3,1) matrix holding 3 partial derivatives --\n",
    "    # one for each feature -- representing the aggregate\n",
    "    # slope of the cost function across all observations\n",
    "    gradient = np.dot(features.T,predictions-target.values)\n",
    "\n",
    "    #3 Take the average cost derivative for each feature\n",
    "    \n",
    "    #4 - Multiply the gradient by our learning rate\n",
    "    gradient/=N\n",
    "    gradient*=lr\n",
    "    \n",
    "\n",
    "    #5 - Subtract from our weights to minimize cost\n",
    "    weights -= gradient\n",
    "\n",
    "    return weights,gradient\n",
    "\n",
    "def train(features, target, weights, lr, iters):\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(iters):\n",
    "        weights,gradient = update_weights(features, target, weights, lr)\n",
    "\n",
    "        #Calculate error for auditing purposes\n",
    "        cost = cost_function(features, target, weights)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Log Progress\n",
    "        if i % 100 == 0:\n",
    "            print(i,cost,gradient)\n",
    "\n",
    "    return weights, cost_history\n",
    "weight=[0,0,0]\n",
    "wt=pd.DataFrame(weight)\n",
    "X=['gmat','gpa','work_experience']\n",
    "y=['admitted']\n",
    "train(df[X],df[y],wt,0.005,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The cost function calculates the cost or error for logistic regression predictions -so we can see how the error changes with change in logistic regression coeefficients and from the function we can get the minimum error after some iterations and we can use coefficient estimates for the corresponding value of the cost estimate to estimate best set of coefficient estimates. The update_weights function changes coefficients using the gradient of the cost function to estimate the best set of coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questionarrie :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. What does Oddsratio>1 imply?\n",
    "\n",
    "2. What is complete separation and quasi complete separation problem in Logistic regression?\n",
    "\n",
    "3. Why can't we use the same cost function of MSE in case of logistic regression as we did in case of linear regression?\n",
    "\n",
    "4. What is the role of VIF(Variable inflation factor) in logistic regression variable selection process?\n",
    "\n",
    "5. In what situation Oversampling is used in modelling process?\n",
    "\n",
    "6. What are the disadvantages for Logistic regression?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Solutions](Solutions.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
